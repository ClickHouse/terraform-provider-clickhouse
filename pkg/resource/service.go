package resource

import (
	"context"
	"crypto/sha1" // nolint:gosec
	"crypto/sha256"
	_ "embed"
	"encoding/base64"
	"encoding/hex"
	"errors"
	"regexp"
	"strings"

	"github.com/hashicorp/terraform-plugin-framework-validators/boolvalidator"
	"github.com/hashicorp/terraform-plugin-framework-validators/int32validator"
	"github.com/hashicorp/terraform-plugin-framework-validators/objectvalidator"
	"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator"
	"github.com/hashicorp/terraform-plugin-framework/attr"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/boolplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int32default"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int32planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/int64planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types/basetypes"

	"github.com/ClickHouse/terraform-provider-clickhouse/pkg/internal/api"
	internalplanmodifier "github.com/ClickHouse/terraform-provider-clickhouse/pkg/internal/planmodifier"
	"github.com/ClickHouse/terraform-provider-clickhouse/pkg/resource/models"

	"github.com/hashicorp/terraform-plugin-framework/path"
	"github.com/hashicorp/terraform-plugin-framework/resource"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/objectplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/types"
)

// Ensure the implementation satisfies the expected interfaces.
var (
	_ resource.Resource                = &ServiceResource{}
	_ resource.ResourceWithConfigure   = &ServiceResource{}
	_ resource.ResourceWithImportState = &ServiceResource{}
	_ resource.ResourceWithModifyPlan  = &ServiceResource{}
)

//go:embed descriptions/service.md
var serviceResourceDescription string

// NewServiceResource is a helper function to simplify the provider implementation.
func NewServiceResource() resource.Resource {
	return &ServiceResource{}
}

// ServiceResource is the resource implementation.
type ServiceResource struct {
	client api.Client
}

// Metadata returns the resource type name.
func (r *ServiceResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_service"
}

// Schema defines the schema for the resource.
func (r *ServiceResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) {
	resp.Schema = schema.Schema{
		Attributes: map[string]schema.Attribute{
			"id": schema.StringAttribute{
				Description: "ID of the created service. Generated by ClickHouse Cloud.",
				Computed:    true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"backup_id": schema.StringAttribute{
				Description: "ID of the backup to restore when creating new service. If specified, the service will be created as a restore operation",
				Optional:    true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.RequiresReplace(),
				},
			},
			"byoc_id": schema.StringAttribute{
				Description: "BYOC ID related to the cloud provider account you want to create this service into.",
				Optional:    true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"warehouse_id": schema.StringAttribute{
				Description: "Set it to the 'warehouse_id' attribute of another service to share the data with it. The service must be in the same cloud and region.",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
					stringplanmodifier.RequiresReplace(),
				},
				Validators: []validator.String{
					stringvalidator.ConflictsWith(path.Expressions{
						path.MatchRoot("password"),
						path.MatchRoot("password_hash"),
						path.MatchRoot("backup_configuration"),
					}...),
				},
			},
			"readonly": schema.BoolAttribute{
				Description: "Indicates if this service should be read only. Only allowed for secondary services, those which share data with another service (i.e. when `warehouse_id` field is set).",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.Bool{
					boolplanmodifier.UseStateForUnknown(),
					boolplanmodifier.RequiresReplace(),
				},
				Validators: []validator.Bool{
					boolvalidator.AlsoRequires(path.Expressions{path.MatchRoot("warehouse_id")}...),
				},
			},
			"is_primary": schema.BoolAttribute{
				Description: "If true, it indicates this is a primary service using its own data. If false it means this service is a secondary service, thus using data from a warehouse.",
				Computed:    true,
				PlanModifiers: []planmodifier.Bool{
					boolplanmodifier.UseStateForUnknown(),
				},
			},
			"name": schema.StringAttribute{
				Description: "User defined identifier for the service.",
				Required:    true,
			},
			"password": schema.StringAttribute{
				Description: "Password for the default user. One of either `password` or `password_hash` must be specified.",
				Optional:    true,
				Sensitive:   true,
				Validators: []validator.String{
					stringvalidator.ConflictsWith(path.Expressions{path.MatchRoot("double_sha1_password_hash")}...),
					stringvalidator.AtLeastOneOf(path.Expressions{
						path.MatchRoot("password_hash"),
						path.MatchRoot("warehouse_id"),
					}...),
				},
			},
			"password_hash": schema.StringAttribute{
				Description: "SHA256 hash of password for the default user. One of either `password` or `password_hash` must be specified.",
				Optional:    true,
				Sensitive:   true,
				Validators: []validator.String{
					stringvalidator.RegexMatches(
						regexp.MustCompile(`^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$`),
						"must be a base64 encoded hash",
					),
					stringvalidator.ConflictsWith(path.Expressions{path.MatchRoot("password")}...),
				},
			},
			"double_sha1_password_hash": schema.StringAttribute{
				Description: "Double SHA1 hash of password for connecting with the MySQL protocol. Cannot be specified if `password` is specified.",
				Optional:    true,
				Sensitive:   true,
				Validators: []validator.String{
					stringvalidator.RegexMatches(
						regexp.MustCompile(`^[0-9a-fA-F]{40}$`),
						"must be a double sha1 hash",
					),
					stringvalidator.AlsoRequires(path.Expressions{path.MatchRoot("password_hash")}...),
				},
			},
			"cloud_provider": schema.StringAttribute{
				Description: "Cloud provider ('aws', 'gcp', or 'azure') in which the service is deployed in.",
				Required:    true,
				Validators: []validator.String{
					stringvalidator.OneOf("aws", "gcp", "azure"),
				},
			},
			"region": schema.StringAttribute{
				Description: "Region within the cloud provider in which the service is deployed in.",
				Required:    true,
			},
			"tier": schema.StringAttribute{
				Description: "Tier of the service: 'development', 'production'. Required for organizations using the Legacy ClickHouse Cloud Tiers, must be omitted for organizations using the new ClickHouse Cloud Tiers.",
				Optional:    true,
				Validators: []validator.String{
					stringvalidator.OneOf(api.TierDevelopment, api.TierProduction),
				},
			},
			"release_channel": schema.StringAttribute{
				Description: "Release channel to use for this service. Can be 'default', 'fast' or 'slow'.",
				Optional:    true,
				Computed:    true,
				Validators: []validator.String{
					stringvalidator.OneOf(api.ReleaseChannelSlow, api.ReleaseChannelDefault, api.ReleaseChannelFast),
				},
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"idle_scaling": schema.BoolAttribute{
				Description: "When set to true the service is allowed to scale down to zero when idle.",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.Bool{
					boolplanmodifier.UseStateForUnknown(),
				},
			},
			"ip_access": schema.ListNestedAttribute{
				Description: "List of IP addresses allowed to access the service.",
				Required:    true,
				NestedObject: schema.NestedAttributeObject{
					Attributes: map[string]schema.Attribute{
						"source": schema.StringAttribute{
							Description: "IP address allowed to access the service. In case you want to set the ip_access to anywhere you should set source to 0.0.0.0/0",
							Required:    true,
						},
						"description": schema.StringAttribute{
							Description: "Description of the IP address.",
							Required:    true,
						},
					},
				},
			},
			"endpoints": schema.SingleNestedAttribute{
				Description: "Allow to enable and configure additional endpoints (read protocols) to expose on the ClickHouse service.",
				Optional:    true,
				Computed:    true,
				Attributes: map[string]schema.Attribute{
					"nativesecure": schema.SingleNestedAttribute{
						Attributes: map[string]schema.Attribute{
							"host": schema.StringAttribute{
								Description: "Endpoint host.",
								Computed:    true,
								PlanModifiers: []planmodifier.String{
									stringplanmodifier.UseStateForUnknown(),
								},
							},
							"port": schema.Int32Attribute{
								Description: "Endpoint port.",
								Computed:    true,
								PlanModifiers: []planmodifier.Int32{
									int32planmodifier.UseStateForUnknown(),
								},
							},
						},
						Optional: true,
						Computed: true,
						PlanModifiers: []planmodifier.Object{
							objectplanmodifier.UseStateForUnknown(),
						},
					},
					"https": schema.SingleNestedAttribute{
						Attributes: map[string]schema.Attribute{
							"host": schema.StringAttribute{
								Description: "Endpoint host.",
								Computed:    true,
								PlanModifiers: []planmodifier.String{
									stringplanmodifier.UseStateForUnknown(),
								},
							},
							"port": schema.Int32Attribute{
								Description: "Endpoint port.",
								Computed:    true,
								PlanModifiers: []planmodifier.Int32{
									int32planmodifier.UseStateForUnknown(),
								},
							},
						},
						Optional: true,
						Computed: true,
						PlanModifiers: []planmodifier.Object{
							objectplanmodifier.UseStateForUnknown(),
						},
					},
					"mysql": schema.SingleNestedAttribute{
						Attributes: map[string]schema.Attribute{
							"enabled": schema.BoolAttribute{
								Required:    true,
								Description: "Wether to enable the mysql endpoint or not.",
							},
							"host": schema.StringAttribute{
								Description: "Endpoint host.",
								Computed:    true,
							},
							"port": schema.Int32Attribute{
								Description: "Endpoint port.",
								Computed:    true,
							},
						},
						Optional: true,
					},
				},
				PlanModifiers: []planmodifier.Object{
					internalplanmodifier.UseStateForUnknownExcept(map[string]map[string]attr.Type{
						"mysql": models.OptionalEndpoint{}.ObjectType().AttrTypes,
					}),
				},
			},
			"min_total_memory_gb": schema.Int64Attribute{
				Description:        "Minimum total memory of all workers during auto-scaling in Gb. Must be a multiple of 12 and greater than 24.",
				Optional:           true,
				DeprecationMessage: "Please use min_replica_memory_gb instead",
			},
			"max_total_memory_gb": schema.Int64Attribute{
				Description:        "Maximum total memory of all workers during auto-scaling in Gb. Must be a multiple of 12 and lower than 360 for non paid services or 720 for paid services.",
				Optional:           true,
				DeprecationMessage: "Please use max_replica_memory_gb instead",
			},
			"min_replica_memory_gb": schema.Int64Attribute{
				Description: "Minimum memory of a single replica during auto-scaling in Gb. Must be a multiple of 4 greater than or equal to 8. `min_replica_memory_gb` x `num_replicas` (default 3) must be lower than 360 for non paid services or 720 for paid services.",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.Int64{
					int64planmodifier.UseStateForUnknown(),
				},
			},
			"max_replica_memory_gb": schema.Int64Attribute{
				Description: "Maximum memory of a single replica during auto-scaling in Gb. Must be a multiple of 4 greater than or equal to 8. `max_replica_memory_gb` x `num_replicas` (default 3) must be lower than 360 for non paid services or 720 for paid services.",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.Int64{
					int64planmodifier.UseStateForUnknown(),
				},
			},
			"num_replicas": schema.Int64Attribute{
				Optional:    true,
				Computed:    true,
				Description: "Number of replicas for the service. Must be between 3 and 20. Contact support to enable this feature.",
				PlanModifiers: []planmodifier.Int64{
					int64planmodifier.UseStateForUnknown(),
				},
			},
			"idle_timeout_minutes": schema.Int64Attribute{
				Description: "Set minimum idling timeout (in minutes). Must be greater than or equal to 5 minutes. Must be set if idle_scaling is enabled.",
				Optional:    true,
				Computed:    true,
				PlanModifiers: []planmodifier.Int64{
					int64planmodifier.UseStateForUnknown(),
				},
			},
			"iam_role": schema.StringAttribute{
				Description: "IAM role used for accessing objects in s3.",
				Computed:    true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
			"private_endpoint_config": schema.SingleNestedAttribute{
				Description: "Service config for private endpoints",
				Computed:    true,
				Attributes: map[string]schema.Attribute{
					"endpoint_service_id": schema.StringAttribute{
						Description: "Unique identifier of the interface endpoint you created in your VPC with the AWS(Service Name) or GCP(Target Service) resource.",
						Computed:    true,
					},
					"private_dns_hostname": schema.StringAttribute{
						Description: "Private DNS Hostname of the VPC you created.",
						Computed:    true,
					},
				},
				PlanModifiers: []planmodifier.Object{
					objectplanmodifier.UseStateForUnknown(),
				},
			},
			"encryption_key": schema.StringAttribute{
				Description: "Custom encryption key ARN.",
				Optional:    true,
				Computed:    true,
			},
			"encryption_assumed_role_identifier": schema.StringAttribute{
				Description: "Custom role identifier ARN.",
				Optional:    true,
			},
			"transparent_data_encryption": schema.SingleNestedAttribute{
				Description: "Configuration of the Transparent Data Encryption (TDE) feature. Requires an organization with the Enterprise plan.",
				Optional:    true,
				Computed:    true,
				Attributes: map[string]schema.Attribute{
					"role_id": schema.StringAttribute{
						Computed:    true,
						Description: "ID of Role to be used for granting access to the Encryption Key. This is an ARN for AWS services and a Service Account Identifier for GCP.",
					},
					"enabled": schema.BoolAttribute{
						Optional:    true,
						Computed:    true, // To allow client side defaulting.
						Description: "If true, TDE is enabled for the service.",
					},
				},
				Validators: []validator.Object{
					objectvalidator.ConflictsWith(path.Expressions{path.MatchRoot("warehouse_id")}...),
				},
			},
			"query_api_endpoints": schema.SingleNestedAttribute{
				Description: "Configuration of the query API endpoints feature.",
				Optional:    true,
				Attributes: map[string]schema.Attribute{
					"api_key_ids": schema.ListAttribute{
						ElementType: types.StringType,
						Required:    true,
						Description: "The UUIDs of the API Keys to grant access to the query API.",
					},
					"roles": schema.ListAttribute{
						ElementType: types.StringType,
						Required:    true,
						Description: "The Database role that will be used to run the query.",
					},
					"allowed_origins": schema.StringAttribute{
						Optional:    true,
						Description: "Comma separated list of domain names to be allowed cross-origin resource sharing (CORS) access to the query API. Leave this field empty to restrict access to backend servers only",
						Validators: []validator.String{
							stringvalidator.LengthAtLeast(1),
						},
					},
				},
			},
			"backup_configuration": schema.SingleNestedAttribute{
				Description: "Configuration of service backup settings.",
				Optional:    true,
				Computed:    true,
				Attributes: map[string]schema.Attribute{
					"backup_period_in_hours": schema.Int32Attribute{
						Description: "Interval in hours between each backup.",
						Optional:    true,
						Computed:    true,
						Default:     int32default.StaticInt32(24),
						Validators: []validator.Int32{
							int32validator.OneOf([]int32{6, 8, 12, 16, 20, 24, 36, 48}...),
							int32validator.ConflictsWith(path.MatchRoot("backup_configuration").AtName("backup_start_time")),
						},
					},
					"backup_retention_period_in_hours": schema.Int32Attribute{
						Description: "How long in hours to keep a backup before deleting it.",
						Optional:    true,
						Computed:    true,
						Default:     int32default.StaticInt32(24),
						Validators: []validator.Int32{
							int32validator.OneOf([]int32{24, 48, 72, 96, 120, 144, 168, 336, 504, 672, 720}...),
						},
					},
					"backup_start_time": schema.StringAttribute{
						Optional:    true,
						Description: "Time of the day in UTC that indicates the start time of a 2 hours window to be used for backup. If set, backup_period_in_hours must be null and backups are created once a day.",
						Validators: []validator.String{
							stringvalidator.RegexMatches(
								regexp.MustCompile(`^(?:[0-9]|0[0-9]|1[0-9]|2[0-3]):00$`),
								"must be in HH:00 format",
							),
							stringvalidator.ConflictsWith(path.MatchRoot("backup_configuration").AtName("backup_period_in_hours")),
						},
					},
				},
				PlanModifiers: []planmodifier.Object{
					objectplanmodifier.UseStateForUnknown(),
				},
			},
		},
		MarkdownDescription: serviceResourceDescription,
		Version:             1,
	}
}

// Configure adds the provider configured client to the resource.
func (r *ServiceResource) Configure(_ context.Context, req resource.ConfigureRequest, _ *resource.ConfigureResponse) {
	if req.ProviderData == nil {
		return
	}

	r.client = req.ProviderData.(api.Client)
}

func (r *ServiceResource) ModifyPlan(ctx context.Context, req resource.ModifyPlanRequest, resp *resource.ModifyPlanResponse) {
	if req.Plan.Raw.IsNull() {
		// If the entire plan is null, the resource is planned for destruction.
		return
	}

	var plan, state, config models.ServiceResourceModel
	diags := req.Plan.Get(ctx, &plan)
	resp.Diagnostics.Append(diags...)
	if !req.State.Raw.IsNull() {
		diags = req.State.Get(ctx, &state)
		resp.Diagnostics.Append(diags...)
	}
	if resp.Diagnostics.HasError() {
		return
	}

	if !req.Config.Raw.IsNull() {
		diags = req.Config.Get(ctx, &config)
		resp.Diagnostics.Append(diags...)
	}
	if resp.Diagnostics.HasError() {
		return
	}

	if !req.State.Raw.IsNull() {
		// Validations for updates.
		if !plan.BYOCID.IsNull() && !plan.BYOCID.IsUnknown() && plan.BYOCID != state.BYOCID {
			resp.Diagnostics.AddAttributeError(
				path.Root("byocid"),
				"Invalid Update",
				"ClickHouse does not support changing BYOC ID for a service",
			)
		}

		if plan.BackupID != state.BackupID {
			resp.Diagnostics.AddAttributeError(
				path.Root("backup_id"),
				"Invalid Update",
				"ClickHouse does not support changing Backup ID for a service",
			)
		}

		if !state.BackupID.IsNull() && plan.BackupID != state.BackupID {
			resp.Diagnostics.AddAttributeError(
				path.Root("backup_id"),
				"Invalid Update",
				"ClickHouse does not support changing Backup ID for a service",
			)
		}

		if !plan.CloudProvider.IsNull() && plan.CloudProvider != state.CloudProvider {
			resp.Diagnostics.AddAttributeError(
				path.Root("cloud_provider"),
				"Invalid Update",
				"ClickHouse does not support changing service cloud providers",
			)
		}

		if !plan.Region.IsNull() && plan.Region != state.Region {
			resp.Diagnostics.AddAttributeError(
				path.Root("region"),
				"Invalid Update",
				"ClickHouse does not support changing service regions",
			)
		}

		if plan.Tier != state.Tier {
			// Check if organization tier was changed from ppv1 to ppv2.
			if !plan.Tier.IsNull() && !plan.Tier.IsUnknown() {
				if state.Tier.IsNull() {
					// Plan specifies a tier, but the API returned null for it.
					// This means Organization was switched from ppv1 to ppv2,
					// so we ask the customer to remove the tier field from the .tf file.
					resp.Diagnostics.AddAttributeError(
						path.Root("tier"),
						"Action required",
						"Please remove the `tier` field from the service definition",
					)
				} else {
					// tier was changed in an organization using legacy tier, this is not allowed.
					resp.Diagnostics.AddAttributeError(
						path.Root("tier"),
						"Invalid Update",
						"ClickHouse does not support changing service tiers",
					)
				}
			}
		}

		if !config.EncryptionKey.IsNull() && plan.EncryptionKey != state.EncryptionKey {
			resp.Diagnostics.AddAttributeError(
				path.Root("encryption_key"),
				"Invalid Update",
				"ClickHouse does not support changing encryption_key",
			)
		}

		if !plan.EncryptionAssumedRoleIdentifier.IsNull() && plan.EncryptionAssumedRoleIdentifier != state.EncryptionAssumedRoleIdentifier {
			resp.Diagnostics.AddAttributeError(
				path.Root("encryption_assumed_role_identifier"),
				"Invalid Update",
				"ClickHouse does not support changing encryption_assumed_role_identifier",
			)
		}

		var isEnabled, wantEnabled bool
		{
			if !state.TransparentEncryptionData.IsNull() {
				stateTDE := models.TransparentEncryptionData{}
				state.TransparentEncryptionData.As(ctx, &stateTDE, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
				isEnabled = stateTDE.Enabled.ValueBool()
			} else {
				isEnabled = false
			}

			if !config.TransparentEncryptionData.IsNull() && !config.TransparentEncryptionData.IsUnknown() {
				configTDE := models.TransparentEncryptionData{}
				config.TransparentEncryptionData.As(ctx, &configTDE, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
				wantEnabled = configTDE.Enabled.ValueBool()
			} else {
				wantEnabled = false
			}
		}

		// Attempt to disable TDE.
		if isEnabled && !wantEnabled {
			resp.Diagnostics.AddAttributeError(
				path.Root("transparent_data_encryption.enabled"),
				"Invalid Update",
				"This service has TDE enabled, but your clickhouse_service resource is not setting it as enabled. Please ensure you have set the transparent_data_encryption.enabled attribute to true",
			)
		}

		// Attempt to enable TDE.
		if !isEnabled && wantEnabled {
			resp.Diagnostics.AddAttributeError(
				path.Root("transparent_data_encryption.enabled"),
				"Invalid Update",
				"It is not possible to enable TDE (Transparent data encryption) on an existing service.",
			)
		}

		if config.IdleTimeoutMinutes.IsNull() {
			plan.IdleTimeoutMinutes = types.Int64Null()
			resp.Plan.Set(ctx, plan)
		}

		if !state.IdleScaling.ValueBool() && plan.IdleScaling.ValueBool() {
			plan.IdleTimeoutMinutes = config.IdleTimeoutMinutes
		}
	}

	if plan.Tier.ValueString() == api.TierDevelopment {
		if !plan.BYOCID.IsNull() && !plan.BYOCID.IsUnknown() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"byoc_id cannot be defined if the service tier is development",
			)
		}

		if !plan.MinTotalMemoryGb.IsNull() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"min_total_memory_gb cannot be defined if the service tier is development",
			)
		}

		if !plan.MaxTotalMemoryGb.IsNull() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"max_total_memory_gb cannot be defined if the service tier is development",
			)
		}

		if !plan.NumReplicas.IsNull() && !plan.NumReplicas.IsUnknown() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"num_replicas cannot be defined if the service tier is development",
			)
		}

		if (!plan.MinReplicaMemoryGb.IsNull() && !plan.MinReplicaMemoryGb.IsUnknown()) ||
			(!plan.MaxReplicaMemoryGb.IsNull() && !plan.MaxReplicaMemoryGb.IsUnknown()) {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"min_replica_memory_gb and max_replica_memory_gb cannot be defined if the service tier is development",
			)
		}

		if !plan.EncryptionKey.IsNull() || !plan.EncryptionAssumedRoleIdentifier.IsNull() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"encryption_key and encryption_assumed_role_identifier cannot be defined if the service tier is development",
			)
		}

		if !plan.BackupConfiguration.IsNull() && !plan.BackupConfiguration.IsUnknown() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"backup_configuration cannot be defined if the service tier is development",
			)
		}

		if !plan.ReleaseChannel.IsUnknown() && plan.ReleaseChannel.ValueString() != api.ReleaseChannelDefault {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"release_channel must be 'default' if the service tier is development",
			)
		}
	} else {
		// Production and PPv2
		if !plan.BYOCID.IsNull() {
			if plan.MinReplicaMemoryGb.IsNull() || plan.MinReplicaMemoryGb.IsUnknown() {
				resp.Diagnostics.AddError(
					"Invalid Configuration",
					"min_replica_memory_gb must be defined if byoc_id is set",
				)
			}

			if plan.MaxReplicaMemoryGb.IsNull() || plan.MaxReplicaMemoryGb.IsUnknown() {
				resp.Diagnostics.AddError(
					"Invalid Configuration",
					"max_replica_memory_gb must be defined if byoc_id is set",
				)
			}
		}

		if plan.MinReplicaMemoryGb.IsNull() && plan.MinTotalMemoryGb.IsNull() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"min_replica_memory_gb must be defined if the service tier is production",
			)
		}

		if plan.MaxReplicaMemoryGb.IsUnknown() && plan.MaxTotalMemoryGb.IsUnknown() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"max_replica_memory_gb must be defined if the service tier is production",
			)
		}

		if !plan.EncryptionAssumedRoleIdentifier.IsNull() && plan.EncryptionKey.IsNull() {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"encryption_assumed_role_identifier cannot be defined without encryption_key as well",
			)
		}

		if !plan.EncryptionKey.IsNull() && strings.Compare(plan.CloudProvider.ValueString(), "aws") != 0 {
			resp.Diagnostics.AddError(
				"Invalid Configuration",
				"encryption_key and the encryption_assumed_role_identifier is only available for aws services",
			)
		}

		if !plan.BackupConfiguration.IsNull() && !plan.BackupConfiguration.IsUnknown() {
			bc := models.BackupConfiguration{}
			diag := plan.BackupConfiguration.As(ctx, &bc, basetypes.ObjectAsOptions{
				UnhandledNullAsEmpty:    false,
				UnhandledUnknownAsEmpty: false,
			})
			if diag.HasError() {
				resp.Diagnostics.Append(diag.Errors()...)
			} else {
				if !config.BackupConfiguration.IsNull() && !config.BackupConfiguration.IsUnknown() {
					cfgBackupConfig := models.BackupConfiguration{}
					diag := config.BackupConfiguration.As(ctx, &cfgBackupConfig, basetypes.ObjectAsOptions{
						UnhandledNullAsEmpty:    false,
						UnhandledUnknownAsEmpty: false,
					})
					if !diag.HasError() {
						if cfgBackupConfig.BackupStartTime.IsNull() || cfgBackupConfig.BackupStartTime.IsUnknown() {
							// Make BackupStartTime null if user only set BackupPeriodInHours.
							bc.BackupStartTime = types.StringNull()
							plan.BackupConfiguration = bc.ObjectValue()
							resp.Plan.Set(ctx, plan)
						}
					}
				}
			}
		}

		// CMEK->TDE migration.
		// if config.encryption_key is null, we need to wipe it out from the state even if the API returns it.
		// This happens when a service is migrated from CMEK to TDE.
		if config.EncryptionKey.IsNull() {
			if !state.EncryptionKey.IsNull() {
				plan.EncryptionKey = state.EncryptionKey
				resp.Plan.Set(ctx, plan)
			} else {
				plan.EncryptionKey = types.StringNull()
			}
		}
	}

	if !plan.MinTotalMemoryGb.IsNull() && !plan.MinReplicaMemoryGb.IsUnknown() {
		resp.Diagnostics.AddError(
			"Invalid Configuration",
			"min_total_memory_gb and min_replica_memory_gb can't be specified at the same time. Please remove deprecated field min_total_memory_gb",
		)
	}

	if !plan.MaxTotalMemoryGb.IsNull() && !plan.MaxReplicaMemoryGb.IsUnknown() {
		resp.Diagnostics.AddError(
			"Invalid Configuration",
			"max_total_memory_gb and max_replica_memory_gb can't be specified at the same time. Please remove deprecated field max_total_memory_gb",
		)
	}

	if (!plan.MinReplicaMemoryGb.IsUnknown() || !plan.MaxReplicaMemoryGb.IsUnknown()) && (!plan.MinTotalMemoryGb.IsNull() || !plan.MaxTotalMemoryGb.IsNull()) {
		resp.Diagnostics.AddError(
			"Invalid Configuration",
			"If you specify either min_replica_memory_gb or max_replica_memory_gb fields, you can't use deprecated min_total_memory_gb nor max_total_memory_gb fields any more.",
		)
	}

	if config.IdleTimeoutMinutes.IsNull() && !config.IdleScaling.IsNull() && config.IdleScaling.ValueBool() {
		resp.Diagnostics.AddError(
			"Invalid Configuration",
			"idle_timeout_minutes must be defined if idle_scaling is enabled",
		)
	}

	if !config.IdleTimeoutMinutes.IsNull() && !config.IdleScaling.IsNull() && !plan.IdleScaling.ValueBool() {
		resp.Diagnostics.AddError(
			"Invalid Configuration",
			"idle_timeout_minutes must be null if idle_scaling is disabled",
		)
	}

	if config.Endpoints.IsNull() || config.Endpoints.IsUnknown() {
		// User did not set the endpoints attribute
		if state.Endpoints.IsNull() || state.Endpoints.IsUnknown() {
			// State is not set, we leave the plan as-is as value is currently unknown.
		} else {
			// State is set and user didn't set (or removed) attribute from the service.
			endpoints := models.Endpoints{}
			diag := state.Endpoints.As(ctx, &endpoints, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			mysql := models.OptionalEndpoint{}
			diag = endpoints.MySQL.As(ctx, &mysql, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			if mysql.Enabled.ValueBool() {
				// Mysql protocol was enabled outside terraform.
				// The default is false, so in the plan we show the need to disable it.
				mysql.Enabled = types.BoolValue(false)
				mysql.Host = types.StringNull()
				mysql.Port = types.Int32Null()
				endpoints.MySQL = mysql.ObjectValue()
				plan.Endpoints = endpoints.ObjectValue()

			} else {
				// All good, no change so we copy Endpoints from the state to the Plan to show no changes.
				plan.Endpoints = state.Endpoints
			}

			resp.Plan.Set(ctx, plan)
		}
	} else {
		// User has Endpoint config set

		var wantEnabled bool
		{
			endpoints := models.Endpoints{}
			diag := config.Endpoints.As(ctx, &endpoints, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			mysql := models.OptionalEndpoint{}
			diag = endpoints.MySQL.As(ctx, &mysql, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			wantEnabled = mysql.Enabled.ValueBool()
		}

		var isEnabled bool
		if !req.State.Raw.IsNull() {
			endpoints := models.Endpoints{}
			diag := state.Endpoints.As(ctx, &endpoints, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			mysql := models.OptionalEndpoint{}
			diag = endpoints.MySQL.As(ctx, &mysql, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
			if diag.HasError() {
				return
			}

			isEnabled = mysql.Enabled.ValueBool()
		}

		if wantEnabled && isEnabled {
			// User did not change wantEnabled so there is no reason to change anything in the Endpoints field.
			plan.Endpoints = state.Endpoints
			resp.Plan.Set(ctx, plan)
		}
	}

	{
		// Default value if there is no TDE attribute in the state.
		tde := models.TransparentEncryptionData{
			Enabled: types.BoolValue(false),
			RoleID:  types.StringNull(),
		}

		// Read the config
		if !config.TransparentEncryptionData.IsNull() {
			configTDE := models.TransparentEncryptionData{}
			config.TransparentEncryptionData.As(ctx, &configTDE, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})

			tde.Enabled = configTDE.Enabled
		}

		// Read the Role ID from the state if set
		if !req.State.Raw.IsNull() && !state.TransparentEncryptionData.IsNull() {
			stateTDE := models.TransparentEncryptionData{}
			state.TransparentEncryptionData.As(ctx, &stateTDE, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})

			tde.RoleID = stateTDE.RoleID
		}

		plan.TransparentEncryptionData = tde.ObjectValue()
		resp.Plan.Set(ctx, plan)
	}

	if !config.DataWarehouseID.IsNull() {
		plan.BackupConfiguration = types.ObjectNull(models.BackupConfiguration{}.ObjectType().AttrTypes)
		resp.Plan.Set(ctx, plan)
	}
}

// Create a new resource
func (r *ServiceResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {
	// Retrieve values from plan
	var plan models.ServiceResourceModel
	diags := req.Plan.Get(ctx, &plan)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Generate API request body from plan
	service := api.Service{
		Name:     plan.Name.ValueString(),
		Provider: plan.CloudProvider.ValueString(),
		Region:   plan.Region.ValueString(),
		Tier:     plan.Tier.ValueString(),
	}

	if !plan.BYOCID.IsUnknown() && !plan.BYOCID.IsNull() {
		service.BYOCId = plan.BYOCID.ValueStringPointer()
	}

	if !plan.BackupID.IsUnknown() && !plan.BackupID.IsNull() {
		service.BackupID = plan.BackupID.ValueStringPointer()
	}

	if !plan.ReleaseChannel.IsUnknown() && !plan.ReleaseChannel.IsNull() {
		service.ReleaseChannel = plan.ReleaseChannel.ValueString()
	}

	if !plan.DataWarehouseID.IsUnknown() && !plan.DataWarehouseID.IsNull() {
		service.DataWarehouseId = plan.DataWarehouseID.ValueStringPointer()
		service.ReadOnly = plan.ReadOnly.ValueBool()
	}

	if service.Tier == api.TierProduction || service.Tier == api.TierPPv2 {
		var minReplicaMemoryGb, maxReplicaMemoryGb int
		if !plan.MinReplicaMemoryGb.IsUnknown() {
			minReplicaMemoryGb = int(plan.MinReplicaMemoryGb.ValueInt64())
		} else {
			// Due to a bug on the API, we always assumed the MinTotalMemoryGb value was always related to 3 replicas.
			// Now we use a per-replica API to set the min total memory so we need to divide by 3 to get the same
			// behaviour as before.
			minReplicaMemoryGb = int(plan.MinTotalMemoryGb.ValueInt64() / 3)
		}

		if !plan.MaxReplicaMemoryGb.IsUnknown() {
			maxReplicaMemoryGb = int(plan.MaxReplicaMemoryGb.ValueInt64())
		} else {
			// Due to a bug on the API, we always assumed the MaxTotalMemoryGb value was always related to 3 replicas.
			// Now we use a per-replica API to set the min total memory so we need to divide by 3 to get the same
			// behaviour as before.
			maxReplicaMemoryGb = int(plan.MaxTotalMemoryGb.ValueInt64() / 3)
		}
		if !plan.NumReplicas.IsNull() {
			numReplicas := int(plan.NumReplicas.ValueInt64())
			if numReplicas > 0 {
				service.NumReplicas = &numReplicas
			}
		}

		service.MinReplicaMemoryGb = &minReplicaMemoryGb
		service.MaxReplicaMemoryGb = &maxReplicaMemoryGb

		if !plan.EncryptionKey.IsNull() {
			service.EncryptionKey = plan.EncryptionKey.ValueString()
		}
		if !plan.EncryptionAssumedRoleIdentifier.IsNull() {
			service.EncryptionAssumedRoleIdentifier = plan.EncryptionAssumedRoleIdentifier.ValueString()
		}
	}

	if !plan.TransparentEncryptionData.IsNull() {
		tde := &models.TransparentEncryptionData{}
		diag := plan.TransparentEncryptionData.As(ctx, tde, basetypes.ObjectAsOptions{UnhandledNullAsEmpty: false, UnhandledUnknownAsEmpty: false})
		if diag.HasError() {
			return
		}
		service.HasTransparentDataEncryption = tde.Enabled.ValueBool()
	}

	service.IdleScaling = plan.IdleScaling.ValueBool()
	if !plan.IdleTimeoutMinutes.IsNull() && !plan.IdleTimeoutMinutes.IsUnknown() {
		idleTimeoutMinutes := int(plan.IdleTimeoutMinutes.ValueInt64())
		service.IdleTimeoutMinutes = &idleTimeoutMinutes
	}

	ipAccessModels := make([]models.IPAccessList, 0, len(plan.IpAccessList.Elements()))
	plan.IpAccessList.ElementsAs(ctx, &ipAccessModels, false)
	ipAccessLists := make([]api.IpAccess, 0, len(ipAccessModels))
	for _, ipAccessModel := range ipAccessModels {
		ipAccessLists = append(ipAccessLists, api.IpAccess{
			Source:      ipAccessModel.Source.ValueString(),
			Description: ipAccessModel.Description.ValueString(),
		})
	}
	service.IpAccessList = ipAccessLists

	// Endpoints
	if !plan.Endpoints.IsNull() && !plan.Endpoints.IsUnknown() {
		endpointsConfig := models.Endpoints{}
		diag := plan.Endpoints.As(ctx, &endpointsConfig, basetypes.ObjectAsOptions{
			UnhandledNullAsEmpty:    false,
			UnhandledUnknownAsEmpty: false,
		})
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		if !endpointsConfig.MySQL.IsNull() {
			mysql := models.OptionalEndpoint{}
			diag = endpointsConfig.MySQL.As(ctx, &mysql, basetypes.ObjectAsOptions{
				UnhandledNullAsEmpty:    false,
				UnhandledUnknownAsEmpty: false,
			})
			if diag.HasError() {
				resp.Diagnostics.Append(diag.Errors()...)
				return
			}

			if mysql.Enabled.ValueBool() {
				service.Endpoints = append(service.Endpoints, api.Endpoint{
					Protocol: api.EndpointProtocolMysql,
					Enabled:  true,
				})
			}
		}
	}

	// Create new service
	s, _, err := r.client.CreateService(ctx, service)
	if err != nil {
		resp.Diagnostics.AddError(
			"Error creating service",
			"Could not create service, unexpected error: "+err.Error(),
		)
		return
	}

	err = r.client.WaitForServiceState(ctx, s.Id, func(state string) bool { return state != api.StateProvisioning }, 20*60)
	if err != nil {
		resp.Diagnostics.AddError(
			"Error retrieving service state",
			"Could not retrieve service state after creation, unexpected error: "+err.Error(),
		)
		return
	}

	// Password and backup settings are only set on parent instances for hydra services
	if plan.DataWarehouseID.IsUnknown() || plan.DataWarehouseID.IsNull() {
		// Update service password if provided explicitly
		planPassword := plan.Password.ValueString()
		if len(planPassword) > 0 {
			_, err := r.client.UpdateServicePassword(ctx, s.Id, servicePasswordUpdateFromPlainPassword(planPassword))
			if err != nil {
				resp.Diagnostics.AddError(
					"Error setting service password",
					"Could not set service password after creation, unexpected error: "+err.Error(),
				)
				return
			}
		}

		// Update hashed service password if provided explicitly
		if passwordHash, doubleSha1PasswordHash := plan.PasswordHash.ValueString(), plan.DoubleSha1PasswordHash.ValueString(); len(passwordHash) > 0 || len(doubleSha1PasswordHash) > 0 {
			passwordUpdate := api.ServicePasswordUpdate{
				NewPasswordHash: passwordHash,
			}

			if len(doubleSha1PasswordHash) > 0 {
				passwordUpdate.NewDoubleSha1Hash = doubleSha1PasswordHash
			}

			_, err := r.client.UpdateServicePassword(ctx, s.Id, passwordUpdate)
			if err != nil {
				resp.Diagnostics.AddError(
					"Error setting service password",
					"Could not set service password after creation, unexpected error: "+err.Error(),
				)
				return
			}
		}

		// Set backup settings.
		if !plan.BackupConfiguration.IsNull() && !plan.BackupConfiguration.IsUnknown() {
			bc := models.BackupConfiguration{}
			diag := plan.BackupConfiguration.As(ctx, &bc, basetypes.ObjectAsOptions{
				UnhandledNullAsEmpty:    false,
				UnhandledUnknownAsEmpty: false,
			})
			if diag.HasError() {
				return
			}

			var startTime *string
			if !bc.BackupStartTime.IsUnknown() {
				startTime = bc.BackupStartTime.ValueStringPointer()
			}

			_, err = r.client.UpdateBackupConfiguration(ctx, s.Id, api.BackupConfiguration{
				BackupPeriodInHours:          bc.BackupPeriodInHours.ValueInt32Pointer(),
				BackupRetentionPeriodInHours: bc.BackupRetentionPeriodInHours.ValueInt32Pointer(),
				BackupStartTime:              startTime,
			})
			if err != nil {
				resp.Diagnostics.AddError(
					"Error setting service backup configuration",
					"Could not set service backup settings after creation, unexpected error: "+err.Error(),
				)
				return
			}
		}
	}

	// Set query api endpoints
	if !plan.QueryAPIEndpoints.IsNull() {
		qae := models.QueryAPIEndpoints{}
		diag := plan.QueryAPIEndpoints.As(ctx, &qae, basetypes.ObjectAsOptions{
			UnhandledNullAsEmpty:    false,
			UnhandledUnknownAsEmpty: false,
		})
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		roles := make([]string, 0)
		diag = qae.Roles.ElementsAs(ctx, &roles, false)
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		keys := make([]string, 0)
		diag = qae.APIKeyIDs.ElementsAs(ctx, &keys, false)
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		_, err := r.client.CreateQueryEndpoint(ctx, s.Id, api.ServiceQueryEndpoint{
			Roles:          roles,
			OpenApiKeys:    keys,
			AllowedOrigins: qae.AllowedOrigins.ValueString(),
		})
		if err != nil {
			resp.Diagnostics.AddError(
				"Error setting service query API endpoints",
				"Could not set service query API endpoints, unexpected error: "+err.Error(),
			)
			return
		}
	}

	// Map response body to schema and populate Computed attribute values
	plan.ID = types.StringValue(s.Id)
	err = r.syncServiceState(ctx, &plan, true)
	if err != nil {
		resp.Diagnostics.AddError(
			"Error Reading ClickHouse Service",
			"Could not read ClickHouse service id "+plan.ID.ValueString()+": "+err.Error(),
		)
		return
	}

	// Set state to fully populated data
	diags = resp.State.Set(ctx, plan)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}
}

// Read refreshes the Terraform state with the latest data.
func (r *ServiceResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {
	var state models.ServiceResourceModel
	diags := req.State.Get(ctx, &state)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	err := r.syncServiceState(ctx, &state, false)
	if err != nil {
		resp.Diagnostics.AddError(
			"Error Reading ClickHouse Service",
			"Could not read ClickHouse service id "+state.ID.ValueString()+": "+err.Error(),
		)
		return
	}

	if state.ID.IsNull() {
		// Resource was deleted outside terraform
		resp.State.RemoveResource(ctx)
	} else {
		// Set refreshed state
		diags = resp.State.Set(ctx, state)
		resp.Diagnostics.Append(diags...)
	}
}

// Update updates the resource and sets the updated Terraform state on success.
func (r *ServiceResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {
	// Retrieve values from plan
	var plan, state, config models.ServiceResourceModel
	diags := req.Plan.Get(ctx, &plan)
	resp.Diagnostics.Append(diags...)
	diags = req.State.Get(ctx, &state)
	resp.Diagnostics.Append(diags...)
	req.Config.Get(ctx, &config)
	resp.Diagnostics.Append(diags...)

	if resp.Diagnostics.HasError() {
		return
	}

	// Generate API request body from plan
	serviceId := state.ID.ValueString()
	service := api.ServiceUpdate{
		Name:         "",
		IpAccessList: nil,
	}
	serviceChange := false

	if plan.Name != state.Name {
		service.Name = plan.Name.ValueString()
		serviceChange = true
	}

	if plan.ReleaseChannel != state.ReleaseChannel && !plan.ReleaseChannel.IsUnknown() {
		service.ReleaseChannel = plan.ReleaseChannel.ValueString()
		serviceChange = true
	}

	if !plan.IpAccessList.Equal(state.IpAccessList) {
		serviceChange = true
		var currentIPAccessList, desiredIPAccessList []models.IPAccessList
		state.IpAccessList.ElementsAs(ctx, &currentIPAccessList, false)
		plan.IpAccessList.ElementsAs(ctx, &desiredIPAccessList, false)

		var add, remove []api.IpAccess

		for _, ipAccess := range currentIPAccessList {
			remove = append(remove, api.IpAccess{
				Source:      ipAccess.Source.ValueString(),
				Description: ipAccess.Description.ValueString(),
			})
		}

		for _, ipAccess := range desiredIPAccessList {
			add = append(add, api.IpAccess{
				Source:      ipAccess.Source.ValueString(),
				Description: ipAccess.Description.ValueString(),
			})
		}

		service.IpAccessList = &api.IpAccessUpdate{
			Add:    add,
			Remove: remove,
		}
	}

	// Endpoints
	if !plan.Endpoints.Equal(state.Endpoints) {
		serviceChange = true
		if !plan.Endpoints.IsNull() && !plan.Endpoints.IsUnknown() {
			endpointsConfig := models.Endpoints{}
			diag := plan.Endpoints.As(ctx, &endpointsConfig, basetypes.ObjectAsOptions{
				UnhandledNullAsEmpty:    false,
				UnhandledUnknownAsEmpty: false,
			})
			if diag.HasError() {
				resp.Diagnostics.Append(diag.Errors()...)
				return
			}

			if !endpointsConfig.MySQL.IsNull() && !endpointsConfig.MySQL.IsUnknown() {
				mysql := models.OptionalEndpoint{}
				diag = endpointsConfig.MySQL.As(ctx, &mysql, basetypes.ObjectAsOptions{
					UnhandledNullAsEmpty:    false,
					UnhandledUnknownAsEmpty: false,
				})
				if diag.HasError() {
					resp.Diagnostics.Append(diag.Errors()...)
					return
				}

				if mysql.Enabled.ValueBool() {
					service.Endpoints = append(service.Endpoints, api.Endpoint{
						Protocol: api.EndpointProtocolMysql,
						Enabled:  true,
					})
				} else {
					// MySQL endpoint was enabled, but now the enabled flag was set to false.
					// We need to disable mysql endpoint.
					service.Endpoints = append(service.Endpoints, api.Endpoint{
						Protocol: api.EndpointProtocolMysql,
						Enabled:  false,
					})
				}
			} else {
				// MySQL endpoint was enabled, but now the 'mysql' attribute was removed.
				// We need to disable mysql endpoint.
				service.Endpoints = append(service.Endpoints, api.Endpoint{
					Protocol: api.EndpointProtocolMysql,
					Enabled:  false,
				})
			}
		} else {
			// MySQL endpoint was enabled, but now the whole `endpoints_configuration` attribute was removed.
			// We need to disable mysql endpoint.
			service.Endpoints = append(service.Endpoints, api.Endpoint{
				Protocol: api.EndpointProtocolMysql,
				Enabled:  false,
			})
		}
	}

	// Update existing service
	if serviceChange {
		var err error
		_, err = r.client.UpdateService(ctx, serviceId, service)
		if err != nil {
			resp.Diagnostics.AddError(
				"Error Updating ClickHouse Service",
				"Could not update service, unexpected error: "+err.Error(),
			)
			return
		}
	}

	scalingChange := false
	replicaScaling := api.ReplicaScalingUpdate{
		IdleScaling: state.IdleScaling.ValueBoolPointer(),
	}

	if plan.IdleScaling != state.IdleScaling {
		scalingChange = true
		idleScaling := new(bool)
		*idleScaling = plan.IdleScaling.ValueBool()
		replicaScaling.IdleScaling = idleScaling
	}
	if plan.MinTotalMemoryGb != state.MinTotalMemoryGb {
		scalingChange = true
		if !plan.MinTotalMemoryGb.IsNull() {
			// Due to a bug on the API, we always assumed the MinTotalMemoryGb value was always related to 3 replicas.
			// Now we use a per-replica API to set the min total memory so we need to divide by 3 to get the same
			// behaviour as before.
			minTotalMemoryGb := int(plan.MinTotalMemoryGb.ValueInt64()) / 3
			replicaScaling.MinReplicaMemoryGb = &minTotalMemoryGb
		}
	}
	if plan.MaxTotalMemoryGb != state.MaxTotalMemoryGb {
		scalingChange = true
		if !plan.MaxTotalMemoryGb.IsNull() {
			// Due to a bug on the API, we always assumed the MaxTotalMemoryGb value was always related to 3 replicas.
			// Now we use a per-replica API to set the min total memory so we need to divide by 3 to get the same
			// behaviour as before.
			maxTotalMemoryGb := int(plan.MaxTotalMemoryGb.ValueInt64()) / 3
			replicaScaling.MaxReplicaMemoryGb = &maxTotalMemoryGb
		}
	}
	if !plan.MinReplicaMemoryGb.IsUnknown() && plan.MinReplicaMemoryGb != state.MinReplicaMemoryGb {
		scalingChange = true
		if !plan.MinReplicaMemoryGb.IsNull() {
			minReplicaMemoryGb := int(plan.MinReplicaMemoryGb.ValueInt64())
			replicaScaling.MinReplicaMemoryGb = &minReplicaMemoryGb
		}
	}
	if !plan.MaxReplicaMemoryGb.IsUnknown() && plan.MaxReplicaMemoryGb != state.MaxReplicaMemoryGb {
		scalingChange = true
		if !plan.MaxReplicaMemoryGb.IsUnknown() {
			maxReplicaMemoryGb := int(plan.MaxReplicaMemoryGb.ValueInt64())
			replicaScaling.MaxReplicaMemoryGb = &maxReplicaMemoryGb
		}
	}
	if !plan.NumReplicas.IsUnknown() && plan.NumReplicas != state.NumReplicas {
		scalingChange = true

		if !plan.NumReplicas.IsNull() && !plan.NumReplicas.IsUnknown() {
			numReplicas := int(plan.NumReplicas.ValueInt64())
			replicaScaling.NumReplicas = &numReplicas
		}
	}
	if plan.IdleTimeoutMinutes != state.IdleTimeoutMinutes {
		scalingChange = true
		if !plan.IdleTimeoutMinutes.IsNull() {
			idleTimeoutMinutes := int(plan.IdleTimeoutMinutes.ValueInt64())
			replicaScaling.IdleTimeoutMinutes = &idleTimeoutMinutes
		}
	}

	if scalingChange {
		var err error
		_, err = r.client.UpdateReplicaScaling(ctx, serviceId, replicaScaling)
		if err != nil {
			resp.Diagnostics.AddError(
				"Error Updating ClickHouse Service Scaling",
				"Could not update service scaling, unexpected error: "+err.Error(),
			)
			return
		}
	}

	password := plan.Password.ValueString()
	if len(password) > 0 && plan.Password != state.Password {
		password = plan.Password.ValueString()
		_, err := r.client.UpdateServicePassword(ctx, serviceId, servicePasswordUpdateFromPlainPassword(password))
		if err != nil {
			resp.Diagnostics.AddError(
				"Error Updating ClickHouse Service Password",
				"Could not update service password, unexpected error: "+err.Error(),
			)
			return
		}
	} else if !plan.PasswordHash.IsNull() || !plan.DoubleSha1PasswordHash.IsNull() {
		passwordUpdate := api.ServicePasswordUpdate{}

		if !plan.PasswordHash.IsNull() { // change in password hash
			passwordUpdate.NewPasswordHash = plan.PasswordHash.ValueString()
		} else { // no change in password hash, use state value
			passwordUpdate.NewPasswordHash = state.PasswordHash.ValueString()
		}

		if !plan.DoubleSha1PasswordHash.IsNull() { // change in double sha1 password hash
			passwordUpdate.NewDoubleSha1Hash = plan.DoubleSha1PasswordHash.ValueString()
		}

		_, err := r.client.UpdateServicePassword(ctx, serviceId, passwordUpdate)
		if err != nil {
			resp.Diagnostics.AddError(
				"Error Updating ClickHouse Service Password",
				"Could not update service password, unexpected error: "+err.Error(),
			)
			return
		}
	}

	// Update Query API endpoints settings.
	if !plan.QueryAPIEndpoints.IsNull() {
		qae := models.QueryAPIEndpoints{}
		diag := plan.QueryAPIEndpoints.As(ctx, &qae, basetypes.ObjectAsOptions{
			UnhandledNullAsEmpty:    false,
			UnhandledUnknownAsEmpty: false,
		})
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		roles := make([]string, 0)
		diag = qae.Roles.ElementsAs(ctx, &roles, false)
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		keys := make([]string, 0)
		diag = qae.APIKeyIDs.ElementsAs(ctx, &keys, false)
		if diag.HasError() {
			resp.Diagnostics.Append(diag.Errors()...)
			return
		}

		_, err := r.client.CreateQueryEndpoint(ctx, serviceId, api.ServiceQueryEndpoint{
			Roles:          roles,
			OpenApiKeys:    keys,
			AllowedOrigins: qae.AllowedOrigins.ValueString(),
		})
		if err != nil {
			resp.Diagnostics.AddError(
				"Error setting service query API endpoints",
				"Could not set service query API endpoints, unexpected error: "+err.Error(),
			)
			return
		}
	} else if !state.QueryAPIEndpoints.IsNull() {
		// query_api_endpoints was removed from tf file
		// or there was an external change
		err := r.client.DeleteQueryEndpoint(ctx, serviceId)
		if err != nil {
			resp.Diagnostics.AddError(
				"Error deleting service query API endpoints",
				"Could not delete service query API endpoints, unexpected error: "+err.Error(),
			)
			return
		}
	}

	// Set backup settings.
	{
		if !plan.BackupConfiguration.IsNull() && !plan.BackupConfiguration.IsUnknown() {
			bc := models.BackupConfiguration{}
			diag := plan.BackupConfiguration.As(ctx, &bc, basetypes.ObjectAsOptions{
				UnhandledNullAsEmpty:    false,
				UnhandledUnknownAsEmpty: false,
			})
			if diag.HasError() {
				resp.Diagnostics.Append(diag.Errors()...)
				return
			}

			backupConfig := api.BackupConfiguration{
				BackupPeriodInHours:          nil,
				BackupRetentionPeriodInHours: nil,
				BackupStartTime:              nil,
			}

			if !bc.BackupPeriodInHours.IsUnknown() {
				backupConfig.BackupPeriodInHours = bc.BackupPeriodInHours.ValueInt32Pointer()
			}

			if !bc.BackupRetentionPeriodInHours.IsUnknown() {
				backupConfig.BackupRetentionPeriodInHours = bc.BackupRetentionPeriodInHours.ValueInt32Pointer()
			}

			if !bc.BackupStartTime.IsUnknown() {
				backupConfig.BackupStartTime = bc.BackupStartTime.ValueStringPointer()
			}

			_, err := r.client.UpdateBackupConfiguration(ctx, serviceId, backupConfig)
			if err != nil {
				resp.Diagnostics.AddError(
					"Error setting service backup configuration",
					"Could not update service backup settings, unexpected error: "+err.Error(),
				)
				return
			}
		}
	}

	err := r.syncServiceState(ctx, &plan, true)
	if err != nil {
		resp.Diagnostics.AddError(
			"Error syncing service state",
			"Could not sync service state, unexpected error: "+err.Error(),
		)
		return
	}

	// CMEK->TDE migration.
	// if config.encryption_key is null, we need to wipe it out from the state even if the API returns it.
	// This happens when a service is migrated from CMEK to TDE.
	if config.EncryptionKey.IsNull() {
		plan.EncryptionKey = types.StringNull()
	}

	diags = resp.State.Set(ctx, plan)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}
}

// Delete deletes the resource and removes the Terraform state on success.
func (r *ServiceResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {
	// Retrieve values from state
	var state models.ServiceResourceModel
	diags := req.State.Get(ctx, &state)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	// Delete existing order
	_, err := r.client.DeleteService(ctx, state.ID.ValueString())
	if err != nil {
		resp.Diagnostics.AddError(
			"Error Deleting ClickHouse Service",
			"Could not delete service, unexpected error: "+err.Error(),
		)
		return
	}
}

func (r *ServiceResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {
	// Retrieve import ID and save to id attribute
	resource.ImportStatePassthroughID(ctx, path.Root("id"), req, resp)
}

func (r *ServiceResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader {
	return map[int64]resource.StateUpgrader{
		// In version 3.0.0 we changed the `endpoints` field from a list to a map and we removed the `endpoints_configuration` field.
		0: {
			PriorSchema: &schema.Schema{
				Attributes: map[string]schema.Attribute{
					"id": schema.StringAttribute{
						Computed: true,
					},
					"byoc_id": schema.StringAttribute{
						Optional: true,
					},
					"warehouse_id": schema.StringAttribute{
						Optional: true,
						Computed: true,
					},
					"readonly": schema.BoolAttribute{
						Optional: true,
						Computed: true,
					},
					"is_primary": schema.BoolAttribute{
						Computed: true,
					},
					"name": schema.StringAttribute{
						Required: true,
					},
					"password": schema.StringAttribute{
						Optional:  true,
						Sensitive: true,
					},
					"password_hash": schema.StringAttribute{
						Optional:  true,
						Sensitive: true,
					},
					"double_sha1_password_hash": schema.StringAttribute{
						Optional:  true,
						Sensitive: true,
					},
					"cloud_provider": schema.StringAttribute{
						Required: true,
					},
					"region": schema.StringAttribute{
						Required: true,
					},
					"tier": schema.StringAttribute{
						Optional: true,
					},
					"release_channel": schema.StringAttribute{
						Optional: true,
						Computed: true,
					},
					"idle_scaling": schema.BoolAttribute{
						Optional: true,
						Computed: true,
					},
					"ip_access": schema.ListNestedAttribute{
						Required: true,
						NestedObject: schema.NestedAttributeObject{
							Attributes: map[string]schema.Attribute{
								"source": schema.StringAttribute{
									Required: true,
								},
								"description": schema.StringAttribute{
									Required: true,
								},
							},
						},
					},
					"endpoints_configuration": schema.SingleNestedAttribute{
						Optional: true,
						Attributes: map[string]schema.Attribute{
							"mysql": schema.SingleNestedAttribute{
								Attributes: map[string]schema.Attribute{
									"enabled": schema.BoolAttribute{
										Required: true,
									},
								},
								Required: true,
							},
						},
					},
					"endpoints": schema.ListNestedAttribute{
						Computed: true,
						NestedObject: schema.NestedAttributeObject{
							Attributes: map[string]schema.Attribute{
								"protocol": schema.StringAttribute{
									Computed: true,
								},
								"host": schema.StringAttribute{
									Computed: true,
								},
								"port": schema.Int64Attribute{
									Computed: true,
								},
							},
						},
					},
					"min_total_memory_gb": schema.Int64Attribute{
						Optional: true,
					},
					"max_total_memory_gb": schema.Int64Attribute{
						Optional: true,
					},
					"min_replica_memory_gb": schema.Int64Attribute{
						Optional: true,
						Computed: true,
					},
					"max_replica_memory_gb": schema.Int64Attribute{
						Optional: true,
						Computed: true,
					},
					"num_replicas": schema.Int64Attribute{
						Optional: true,
						Computed: true,
					},
					"idle_timeout_minutes": schema.Int64Attribute{
						Optional: true,
					},
					"iam_role": schema.StringAttribute{
						Computed: true,
					},
					"private_endpoint_config": schema.SingleNestedAttribute{
						Computed: true,
						Attributes: map[string]schema.Attribute{
							"endpoint_service_id": schema.StringAttribute{
								Computed: true,
							},
							"private_dns_hostname": schema.StringAttribute{
								Computed: true,
							},
						},
					},
					"encryption_key": schema.StringAttribute{
						Optional: true,
					},
					"encryption_assumed_role_identifier": schema.StringAttribute{
						Optional: true,
					},
					"query_api_endpoints": schema.SingleNestedAttribute{
						Optional: true,
						Attributes: map[string]schema.Attribute{
							"api_key_ids": schema.ListAttribute{
								ElementType: types.StringType,
								Required:    true,
							},
							"roles": schema.ListAttribute{
								ElementType: types.StringType,
								Required:    true,
							},
							"allowed_origins": schema.StringAttribute{
								Optional: true,
							},
						},
					},
					"backup_configuration": schema.SingleNestedAttribute{
						Optional: true,
						Computed: true,
						Attributes: map[string]schema.Attribute{
							"backup_period_in_hours": schema.Int32Attribute{
								Optional: true,
								Computed: true,
							},
							"backup_retention_period_in_hours": schema.Int32Attribute{
								Optional: true,
								Computed: true,
							},
							"backup_start_time": schema.StringAttribute{
								Optional: true,
							},
						},
					},
				},
			},
			StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) {
				type oldService struct {
					ID                              types.String `tfsdk:"id"`
					BYOCID                          types.String `tfsdk:"byoc_id"`
					DataWarehouseID                 types.String `tfsdk:"warehouse_id"`
					IsPrimary                       types.Bool   `tfsdk:"is_primary"`
					ReadOnly                        types.Bool   `tfsdk:"readonly"`
					Name                            types.String `tfsdk:"name"`
					Password                        types.String `tfsdk:"password"`
					PasswordHash                    types.String `tfsdk:"password_hash"`
					DoubleSha1PasswordHash          types.String `tfsdk:"double_sha1_password_hash"`
					EndpointsConfiguration          types.Object `tfsdk:"endpoints_configuration"`
					Endpoints                       types.List   `tfsdk:"endpoints"`
					CloudProvider                   types.String `tfsdk:"cloud_provider"`
					Region                          types.String `tfsdk:"region"`
					Tier                            types.String `tfsdk:"tier"`
					ReleaseChannel                  types.String `tfsdk:"release_channel"`
					IdleScaling                     types.Bool   `tfsdk:"idle_scaling"`
					IpAccessList                    types.List   `tfsdk:"ip_access"`
					MinTotalMemoryGb                types.Int64  `tfsdk:"min_total_memory_gb"`
					MaxTotalMemoryGb                types.Int64  `tfsdk:"max_total_memory_gb"`
					MinReplicaMemoryGb              types.Int64  `tfsdk:"min_replica_memory_gb"`
					MaxReplicaMemoryGb              types.Int64  `tfsdk:"max_replica_memory_gb"`
					NumReplicas                     types.Int64  `tfsdk:"num_replicas"`
					IdleTimeoutMinutes              types.Int64  `tfsdk:"idle_timeout_minutes"`
					IAMRole                         types.String `tfsdk:"iam_role"`
					PrivateEndpointConfig           types.Object `tfsdk:"private_endpoint_config"`
					EncryptionKey                   types.String `tfsdk:"encryption_key"`
					EncryptionAssumedRoleIdentifier types.String `tfsdk:"encryption_assumed_role_identifier"`
					QueryAPIEndpoints               types.Object `tfsdk:"query_api_endpoints"`
					BackupConfiguration             types.Object `tfsdk:"backup_configuration"`
				}

				var priorStateData oldService

				resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...)

				if resp.Diagnostics.HasError() {
					return
				}

				var endpoints models.Endpoints
				{
					endpoints = models.Endpoints{
						NativeSecure: models.Endpoint{
							Host: types.StringNull(),
							Port: types.Int32Null(),
						}.ObjectValue(),
						HTTPS: models.Endpoint{
							Host: types.StringNull(),
							Port: types.Int32Null(),
						}.ObjectValue(),
						MySQL: models.OptionalEndpoint{
							Enabled: types.BoolValue(false),
							Host:    types.StringNull(),
							Port:    types.Int32Null(),
						}.ObjectValue(),
					}

					oldEndpoints := make([]struct {
						Protocol string `tfsdk:"protocol"`
						Host     string `tfsdk:"host"`
						Port     int32  `tfsdk:"port"`
					}, 0)
					diag := priorStateData.Endpoints.ElementsAs(ctx, &oldEndpoints, false)
					if diag.HasError() {
						resp.Diagnostics.Append(diag...)
						return
					}

					for _, ep := range oldEndpoints {
						switch ep.Protocol {
						case api.EndpointProtocolNativeSecure:
							endpoints.NativeSecure = models.Endpoint{
								Host: types.StringValue(ep.Host),
								Port: types.Int32Value(ep.Port),
							}.ObjectValue()
						case api.EndpointProtocolHTTPS:
							endpoints.HTTPS = models.Endpoint{
								Host: types.StringValue(ep.Host),
								Port: types.Int32Value(ep.Port),
							}.ObjectValue()
						case api.EndpointProtocolMysql:
							endpoints.MySQL = models.OptionalEndpoint{
								Enabled: types.BoolValue(true),
								Host:    types.StringValue(ep.Host),
								Port:    types.Int32Value(ep.Port),
							}.ObjectValue()
						}
					}
				}

				upgradedStateData := models.ServiceResourceModel{
					ID:                              priorStateData.ID,
					BYOCID:                          priorStateData.BYOCID,
					BackupID:                        types.StringNull(),
					DataWarehouseID:                 priorStateData.DataWarehouseID,
					IsPrimary:                       priorStateData.IsPrimary,
					ReadOnly:                        priorStateData.ReadOnly,
					Name:                            priorStateData.Name,
					Password:                        priorStateData.Password,
					PasswordHash:                    priorStateData.PasswordHash,
					DoubleSha1PasswordHash:          priorStateData.DoubleSha1PasswordHash,
					Endpoints:                       endpoints.ObjectValue(),
					CloudProvider:                   priorStateData.CloudProvider,
					Region:                          priorStateData.Region,
					Tier:                            priorStateData.Tier,
					ReleaseChannel:                  priorStateData.ReleaseChannel,
					IdleScaling:                     priorStateData.IdleScaling,
					IpAccessList:                    priorStateData.IpAccessList,
					MinTotalMemoryGb:                priorStateData.MinTotalMemoryGb,
					MaxTotalMemoryGb:                priorStateData.MaxTotalMemoryGb,
					MinReplicaMemoryGb:              priorStateData.MinReplicaMemoryGb,
					MaxReplicaMemoryGb:              priorStateData.MaxReplicaMemoryGb,
					NumReplicas:                     priorStateData.NumReplicas,
					IdleTimeoutMinutes:              priorStateData.IdleTimeoutMinutes,
					IAMRole:                         priorStateData.IAMRole,
					PrivateEndpointConfig:           priorStateData.PrivateEndpointConfig,
					EncryptionKey:                   priorStateData.EncryptionKey,
					EncryptionAssumedRoleIdentifier: priorStateData.EncryptionAssumedRoleIdentifier,
					QueryAPIEndpoints:               priorStateData.QueryAPIEndpoints,
					BackupConfiguration:             priorStateData.BackupConfiguration,
					TransparentEncryptionData:       models.TransparentEncryptionData{}.ObjectValue(),
				}

				resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...)
			},
		},
	}
}

// syncServiceState fetches the latest state ClickHouse Cloud API and updates the Terraform state.
func (r *ServiceResource) syncServiceState(ctx context.Context, state *models.ServiceResourceModel, updateTimestamp bool) error {
	if state.ID.IsNull() {
		return errors.New("service ID must be set to fetch the service")
	}

	// Get latest service value from ClickHouse OpenAPI
	service, err := r.client.GetService(ctx, state.ID.ValueString())
	if api.IsNotFound(err) {
		// Service was deleted outside terraform.
		state.ID = types.StringNull()

		return nil
	} else if err != nil {
		return err
	}

	// Overwrite items with refreshed state
	if service.BYOCId != nil {
		state.BYOCID = types.StringValue(*service.BYOCId)
	} else {
		state.BYOCID = types.StringNull()
	}
	if service.DataWarehouseId != nil {
		state.DataWarehouseID = types.StringValue(*service.DataWarehouseId)
	} else {
		state.DataWarehouseID = types.StringNull()
	}
	state.IsPrimary = types.BoolValue(*service.IsPrimary)
	state.ReadOnly = types.BoolValue(service.ReadOnly)
	state.Name = types.StringValue(service.Name)
	state.CloudProvider = types.StringValue(service.Provider)
	state.Region = types.StringValue(service.Region)
	if service.Tier != api.TierPPv2 {
		state.Tier = types.StringValue(service.Tier)
	} else {
		state.Tier = types.StringNull()
	}
	state.ReleaseChannel = types.StringValue(service.ReleaseChannel)
	state.IdleScaling = types.BoolValue(service.IdleScaling)
	if state.IdleScaling.ValueBool() {
		if service.IdleTimeoutMinutes != nil {
			state.IdleTimeoutMinutes = types.Int64Value(int64(*service.IdleTimeoutMinutes))
		}
	} else {
		state.IdleTimeoutMinutes = types.Int64Null()
	}

	if service.Tier == api.TierProduction || service.Tier == api.TierPPv2 {
		if service.MinReplicaMemoryGb != nil {
			state.MinReplicaMemoryGb = types.Int64Value(int64(*service.MinReplicaMemoryGb))
		}
		if service.MaxReplicaMemoryGb != nil {
			state.MaxReplicaMemoryGb = types.Int64Value(int64(*service.MaxReplicaMemoryGb))
		}
		if service.NumReplicas != nil {
			state.NumReplicas = types.Int64Value(int64(*service.NumReplicas))
		}
	} else {
		state.MinReplicaMemoryGb = types.Int64Null()
		state.MaxReplicaMemoryGb = types.Int64Null()
		state.NumReplicas = types.Int64Null()
	}

	{
		var ipAccessList []attr.Value
		for _, ipAccess := range service.IpAccessList {
			ipAccessList = append(ipAccessList, models.IPAccessList{Source: types.StringValue(ipAccess.Source), Description: types.StringValue(ipAccess.Description)}.ObjectValue())
		}
		state.IpAccessList, _ = types.ListValue(models.IPAccessList{}.ObjectType(), ipAccessList)
	}

	{
		endpointsConfiguration := models.Endpoints{
			NativeSecure: models.Endpoint{
				Host: types.StringNull(),
				Port: types.Int32Null(),
			}.ObjectValue(),
			HTTPS: models.Endpoint{
				Host: types.StringNull(),
				Port: types.Int32Null(),
			}.ObjectValue(),
			MySQL: models.OptionalEndpoint{
				Enabled: types.BoolValue(false),
				Host:    types.StringNull(),
				Port:    types.Int32Null(),
			}.ObjectValue(),
		}
		for _, endpoint := range service.Endpoints {
			switch endpoint.Protocol {
			case api.EndpointProtocolNativeSecure:
				endpointsConfiguration.NativeSecure = models.Endpoint{
					Host: types.StringValue(endpoint.Host),
					Port: types.Int32Value(int32(endpoint.Port)), //nolint:gosec
				}.ObjectValue()
			case api.EndpointProtocolHTTPS:
				endpointsConfiguration.HTTPS = models.Endpoint{
					Host: types.StringValue(endpoint.Host),
					Port: types.Int32Value(int32(endpoint.Port)), //nolint:gosec
				}.ObjectValue()
			case api.EndpointProtocolMysql:
				endpointsConfiguration.MySQL = models.OptionalEndpoint{
					Enabled: types.BoolValue(true),
					Host:    types.StringValue(endpoint.Host),
					Port:    types.Int32Value(int32(endpoint.Port)), //nolint:gosec
				}.ObjectValue()
			}
		}
		state.Endpoints = endpointsConfiguration.ObjectValue()
	}

	state.IAMRole = types.StringValue(service.IAMRole)

	state.PrivateEndpointConfig = models.PrivateEndpointConfig{
		EndpointServiceID:  types.StringValue(service.PrivateEndpointConfig.EndpointServiceId),
		PrivateDNSHostname: types.StringValue(service.PrivateEndpointConfig.PrivateDnsHostname),
	}.ObjectValue()

	if service.EncryptionKey != "" {
		state.EncryptionKey = types.StringValue(service.EncryptionKey)
	} else {
		state.EncryptionKey = types.StringNull()
	}
	if service.EncryptionAssumedRoleIdentifier != "" {
		state.EncryptionAssumedRoleIdentifier = types.StringValue(service.EncryptionAssumedRoleIdentifier)
	} else {
		state.EncryptionAssumedRoleIdentifier = types.StringNull()
	}

	tde := models.TransparentEncryptionData{
		Enabled: types.BoolValue(service.HasTransparentDataEncryption),
	}

	if service.EncryptionRoleID != "" {
		tde.RoleID = types.StringValue(service.EncryptionRoleID)
	} else {
		tde.RoleID = types.StringNull()
	}

	state.TransparentEncryptionData = tde.ObjectValue()

	if service.QueryAPIEndpoints != nil {
		queryApiEndpoint := models.QueryAPIEndpoints{}

		apiKeys := make([]attr.Value, 0)
		for _, k := range service.QueryAPIEndpoints.OpenApiKeys {
			apiKeys = append(apiKeys, types.StringValue(k))
		}
		queryApiEndpoint.APIKeyIDs, _ = types.ListValue(types.StringType, apiKeys)

		roles := make([]attr.Value, 0)
		for _, k := range service.QueryAPIEndpoints.Roles {
			roles = append(roles, types.StringValue(k))
		}
		queryApiEndpoint.Roles, _ = types.ListValue(types.StringType, roles)

		if service.QueryAPIEndpoints.AllowedOrigins != "" {
			queryApiEndpoint.AllowedOrigins = types.StringValue(service.QueryAPIEndpoints.AllowedOrigins)
		} else {
			queryApiEndpoint.AllowedOrigins = types.StringNull()
		}

		state.QueryAPIEndpoints = queryApiEndpoint.ObjectValue()
	} else {
		state.QueryAPIEndpoints = types.ObjectNull(models.QueryAPIEndpoints{}.ObjectType().AttrTypes)
	}

	if service.Tier == api.TierProduction || service.Tier == api.TierPPv2 {
		if service.BackupConfiguration != nil {
			backupConfiguration := models.BackupConfiguration{
				BackupPeriodInHours:          types.Int32Null(),
				BackupRetentionPeriodInHours: types.Int32Null(),
				BackupStartTime:              types.StringNull(),
			}

			if service.BackupConfiguration.BackupPeriodInHours != nil && *service.BackupConfiguration.BackupPeriodInHours > 0 {
				backupConfiguration.BackupPeriodInHours = types.Int32Value(*service.BackupConfiguration.BackupPeriodInHours)
			}
			if service.BackupConfiguration.BackupRetentionPeriodInHours != nil && *service.BackupConfiguration.BackupRetentionPeriodInHours > 0 {
				backupConfiguration.BackupRetentionPeriodInHours = types.Int32Value(*service.BackupConfiguration.BackupRetentionPeriodInHours)
			}
			if service.BackupConfiguration.BackupStartTime != nil && *service.BackupConfiguration.BackupStartTime != "" {
				backupConfiguration.BackupStartTime = types.StringValue(*service.BackupConfiguration.BackupStartTime)
			}

			state.BackupConfiguration = backupConfiguration.ObjectValue()
		}
	} else {
		state.BackupConfiguration = types.ObjectNull(models.BackupConfiguration{}.ObjectType().AttrTypes)
	}

	return nil
}

func servicePasswordUpdateFromPlainPassword(password string) api.ServicePasswordUpdate {
	hash := sha256.Sum256([]byte(password))

	singleSha1Hash := sha1.Sum([]byte(password))  // nolint:gosec
	doubleSha1Hash := sha1.Sum(singleSha1Hash[:]) // nolint:gosec

	return api.ServicePasswordUpdate{
		NewPasswordHash:   base64.StdEncoding.EncodeToString(hash[:]),
		NewDoubleSha1Hash: hex.EncodeToString(doubleSha1Hash[:]),
	}
}
