---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "clickhouse_clickpipe Resource - clickhouse"
subcategory: ""
description: |-
  This experimental resource allows you to create and manage ClickPipes data ingestion in ClickHouse Cloud.
  Resource is early access and may change in future releases. Feature coverage might not fully cover all ClickPipe capabilities.
  Known limitations:
  ClickPipe does not support table updates for managed tables. If you need to update the table schema, you will have to do that externally.
---

# clickhouse_clickpipe (Resource)

This experimental resource allows you to create and manage ClickPipes data ingestion in ClickHouse Cloud.

**Resource is early access and may change in future releases. Feature coverage might not fully cover all ClickPipe capabilities.**

Known limitations:
- ClickPipe does not support table updates for managed tables. If you need to update the table schema, you will have to do that externally.

## Example Usage

```terraform
resource "clickhouse_clickpipe" "kafka_clickpipe" {
  name        = "My Kafka ClickPipe"
  description = "Data pipeline from Kafka to ClickHouse"

  service_id = "e9465b4b-f7e5-4937-8e21-8d508b02843d"

  scaling {
    replicas               = 2
    replica_cpu_millicores = 250
    replica_memory_gb      = 1.0
  }

  state = "Running"

  source {
    kafka {
      type    = "confluent"
      format  = "JSONEachRow"
      brokers = "my-kafka-broker:9092"
      topics  = "my_topic"

      consumer_group = "clickpipe-test"

      credentials {
        username = "user"
        password = "***"
      }
    }
  }

  destination {
    table         = "my_table"
    managed_table = true

    tableDefinition {
      engine {
        type = "MergeTree"
      }
    }

    columns {
      name = "my_field1"
      type = "String"
    }

    columns {
      name = "my_field2"
      type = "UInt64"
    }
  }

  field_mappings = [
    {
      source_field      = "my_field"
      destination_field = "my_field1"
    }
  ]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `destination` (Attributes) The destination for the ClickPipe. (see [below for nested schema](#nestedatt--destination))
- `name` (String) The name of the ClickPipe.
- `service_id` (String) The ID of the service to which the ClickPipe belongs.
- `source` (Attributes) The data source for the ClickPipe. At least one source configuration must be provided. (see [below for nested schema](#nestedatt--source))

### Optional

- `field_mappings` (Attributes List) Field mapping between source and destination table. (see [below for nested schema](#nestedatt--field_mappings))
- `scaling` (Attributes) (see [below for nested schema](#nestedatt--scaling))
- `settings` (Dynamic) Advanced configuration options for the ClickPipe. These settings are specific to each pipe. For the complete list of available options, see the [OpenAPI documentation](https://clickhouse.com/docs/cloud/manage/api/swagger#tag/ClickPipes/paths/~1v1~1organizations~1%7BorganizationId%7D~1services~1%7BserviceId%7D~1clickpipes~1%7BclickPipeId%7D~1settings/put)
- `stopped` (Boolean) Whether the ClickPipe should be stopped. Default is `false` (ClickPipe will be running).

### Read-Only

- `id` (String) The ID of the ClickPipe. Generated by the ClickHouse Cloud.
- `state` (String) The current state of the ClickPipe. This is a read-only field that reports the actual state from ClickHouse Cloud. Possible values include `Running`, `Stopped`, `Provisioning`, `Failed`, `InternalError`, etc.

<a id="nestedatt--destination"></a>
### Nested Schema for `destination`

Required:

- `columns` (Attributes List) The list of columns for the ClickHouse table. (see [below for nested schema](#nestedatt--destination--columns))
- `table` (String) The name of the ClickHouse table.

Optional:

- `database` (String) The name of the ClickHouse database. Default is `default`.
- `managed_table` (Boolean) Whether the table is managed by ClickHouse Cloud. If `false`, the table must exist in the database. Default is `true`.
- `roles` (List of String) ClickPipe will create a ClickHouse user with these roles. Add your custom roles here if required.
- `table_definition` (Attributes) Definition of the destination table. Required for ClickPipes managed tables. (see [below for nested schema](#nestedatt--destination--table_definition))

<a id="nestedatt--destination--columns"></a>
### Nested Schema for `destination.columns`

Required:

- `name` (String) The name of the column.
- `type` (String) The type of the column.


<a id="nestedatt--destination--table_definition"></a>
### Nested Schema for `destination.table_definition`

Required:

- `engine` (Attributes) The engine of the ClickHouse table. (see [below for nested schema](#nestedatt--destination--table_definition--engine))

Optional:

- `partition_by` (String) The column to partition the table by.
- `primary_key` (String) The primary key of the table.
- `sorting_key` (List of String) The list of columns for the sorting key.

<a id="nestedatt--destination--table_definition--engine"></a>
### Nested Schema for `destination.table_definition.engine`

Required:

- `type` (String) The type of the engine. Supported engines: `MergeTree`, `ReplacingMergeTree`, `SummingMergeTree`, `Null`.

Optional:

- `column_ids` (List of String) Column IDs to sum for SummingMergeTree engine. Required when engine type is `SummingMergeTree`.
- `version_column_id` (String) Column ID to use as version for ReplacingMergeTree engine. Required when engine type is `ReplacingMergeTree`.




<a id="nestedatt--source"></a>
### Nested Schema for `source`

Optional:

- `kafka` (Attributes) The Kafka source configuration for the ClickPipe. (see [below for nested schema](#nestedatt--source--kafka))
- `kinesis` (Attributes) The Kinesis source configuration for the ClickPipe. (see [below for nested schema](#nestedatt--source--kinesis))
- `object_storage` (Attributes) The compatible object storage source configuration for the ClickPipe. (see [below for nested schema](#nestedatt--source--object_storage))

<a id="nestedatt--source--kafka"></a>
### Nested Schema for `source.kafka`

Required:

- `brokers` (String) The list of Kafka bootstrap brokers. (comma separated)
- `format` (String) The format of the Kafka source. (`JSONEachRow`, `Avro`, `AvroConfluent`)
- `topics` (String) The list of Kafka topics. (comma separated)

Optional:

- `authentication` (String) The authentication method for the Kafka source. (`PLAIN`, `SCRAM-SHA-256`, `SCRAM-SHA-512`, `IAM_ROLE`, `IAM_USER`). Default is `PLAIN`.
- `ca_certificate` (String) PEM encoded CA certificates to validate the broker's certificate.
- `consumer_group` (String) Consumer group of the Kafka source. If not provided `clickpipes-<ID>` will be used.
- `credentials` (Attributes) The credentials for the Kafka source. (see [below for nested schema](#nestedatt--source--kafka--credentials))
- `iam_role` (String) The IAM role for the Kafka source. Use with `IAM_ROLE` authentication. It can be used with AWS ClickHouse service only. Read more in [ClickPipes documentation page](https://clickhouse.com/docs/en/integrations/clickpipes/kafka#iam)
- `offset` (Attributes) The Kafka offset. (see [below for nested schema](#nestedatt--source--kafka--offset))
- `reverse_private_endpoint_ids` (List of String) The list of reverse private endpoint IDs for the Kafka source. (comma separated)
- `schema_registry` (Attributes) The schema registry for the Kafka source. (see [below for nested schema](#nestedatt--source--kafka--schema_registry))
- `type` (String) The type of the Kafka source. (`kafka`, `redpanda`, `confluent`, `msk`, `warpstream`, `azureeventhub`). Default is `kafka`.

<a id="nestedatt--source--kafka--credentials"></a>
### Nested Schema for `source.kafka.credentials`

Optional:

- `access_key_id` (String, Sensitive) The access key ID for the Kafka source. Use with `IAM_USER` authentication.
- `connection_string` (String, Sensitive) The connection string for the Kafka source. Use with `azureeventhub` Kafka source type. Use with `PLAIN` authentication.
- `password` (String, Sensitive) The password for the Kafka source.
- `secret_key` (String, Sensitive) The secret key for the Kafka source. Use with `IAM_USER` authentication.
- `username` (String, Sensitive) The username for the Kafka source.


<a id="nestedatt--source--kafka--offset"></a>
### Nested Schema for `source.kafka.offset`

Required:

- `strategy` (String) The offset strategy for the Kafka source. (`from_beginning`, `from_latest`, `from_timestamp`)

Optional:

- `timestamp` (String) The timestamp for the Kafka offset. Use with `from_timestamp` offset strategy. (format `2021-01-01T00:00`)


<a id="nestedatt--source--kafka--schema_registry"></a>
### Nested Schema for `source.kafka.schema_registry`

Required:

- `authentication` (String) The authentication method for the Schema Registry. Only supported is `PLAIN`.
- `credentials` (Attributes) The credentials for the Schema Registry. (see [below for nested schema](#nestedatt--source--kafka--schema_registry--credentials))
- `url` (String) The URL of the schema registry.

<a id="nestedatt--source--kafka--schema_registry--credentials"></a>
### Nested Schema for `source.kafka.schema_registry.credentials`

Required:

- `password` (String, Sensitive) The password for the Schema Registry.
- `username` (String, Sensitive) The username for the Schema Registry.




<a id="nestedatt--source--kinesis"></a>
### Nested Schema for `source.kinesis`

Required:

- `authentication` (String) The authentication method for the Kinesis source. (`IAM_ROLE`, `IAM_USER`).
- `format` (String) The format of the Kinesis source. (`JSONEachRow`, `Avro`, `AvroConfluent`)
- `iterator_type` (String) The iterator type for the Kinesis source. (`TRIM_HORIZON`, `LATEST`, `AT_TIMESTAMP`)
- `region` (String) The AWS region of the Kinesis stream.
- `stream_name` (String) The name of the Kinesis stream.

Optional:

- `access_key` (Attributes) The access key for the Kinesis source. Use with `IAM_USER` authentication. (see [below for nested schema](#nestedatt--source--kinesis--access_key))
- `iam_role` (String) The IAM role for the Kinesis source. Use with `IAM_ROLE` authentication. It can be used with AWS ClickHouse service only. Read more in [ClickPipes documentation page](https://clickhouse.com/docs/en/integrations/clickpipes/kinesis).
- `timestamp` (String) The timestamp for the Kinesis source. Use with `AT_TIMESTAMP` iterator type. (format `2021-01-01T00:00`)
- `use_enhanced_fan_out` (Boolean) Whether to use enhanced fan-out consumer.

<a id="nestedatt--source--kinesis--access_key"></a>
### Nested Schema for `source.kinesis.access_key`

Required:

- `access_key_id` (String, Sensitive) The access key ID for the Kinesis source.
- `secret_key` (String, Sensitive) The secret key for the Kinesis source.



<a id="nestedatt--source--object_storage"></a>
### Nested Schema for `source.object_storage`

Required:

- `format` (String) The format of the S3 objects. (`JSONEachRow`, `CSV`, `CSVWithNames`, `Parquet`)

Optional:

- `access_key` (Attributes) Access key (see [below for nested schema](#nestedatt--source--object_storage--access_key))
- `authentication` (String) CONNECTION_STRING is for Azure Blob Storage. IAM_ROLE and IAM_USER are for AWS S3/GCS/DigitalOcean. If not provided, no authentication is used
- `azure_container_name` (String) Container name for Azure Blob Storage. Required when type is azureblobstorage. Example: `mycontainer`
- `compression` (String) Compression algorithm used for the files.. (`auto`, `gzip`, `brotli`, `br`, `xz`, `LZMA`, `zstd`)
- `connection_string` (String, Sensitive) Connection string for Azure Blob Storage authentication. Required when authentication is CONNECTION_STRING. Example: `DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey;EndpointSuffix=core.windows.net`
- `delimiter` (String) The delimiter for the S3 source. Default is `,`.
- `iam_role` (String) The IAM role for the S3 source. Use with `IAM_ROLE` authentication. It can be used with AWS ClickHouse service only. Read more in [ClickPipes documentation page](https://clickhouse.com/docs/en/integrations/clickpipes/object-storage#authentication)
- `is_continuous` (Boolean) If set to true, the pipe will continuously read new files from the source. If set to false, the pipe will read the files only once. New files have to be uploaded lexically order.
- `path` (String) Path to the file(s) within the Azure container. Used for Azure Blob Storage sources. You can specify multiple files using bash-like wildcards. For more information, see the documentation on using wildcards in path: https://clickhouse.com/docs/en/integrations/clickpipes/object-storage#limitations. Example: `data/logs/*.json`
- `type` (String) The type of the S3-compatbile source (`s3`, `gcs`, `azureblobstorage`). Default is `s3`.
- `url` (String) The URL of the S3/GCS bucket. Required for S3 and GCS types. Not used for Azure Blob Storage (use path and azure_container_name instead). You can specify multiple files using bash-like wildcards. For more information, see the documentation on using wildcards in path: https://clickhouse.com/docs/en/integrations/clickpipes/object-storage#limitations

<a id="nestedatt--source--object_storage--access_key"></a>
### Nested Schema for `source.object_storage.access_key`

Optional:

- `access_key_id` (String, Sensitive) The access key ID for the S3 source. Use with `IAM_USER` authentication.
- `secret_key` (String, Sensitive) The secret key for the S3 source. Use with `IAM_USER` authentication.




<a id="nestedatt--field_mappings"></a>
### Nested Schema for `field_mappings`

Required:

- `destination_field` (String) The name of the column in destination table.
- `source_field` (String) The name of the source field.


<a id="nestedatt--scaling"></a>
### Nested Schema for `scaling`

Optional:

- `replica_cpu_millicores` (Number) The CPU allocation per replica in millicores. Must be between 125 and 2000.
- `replica_memory_gb` (Number) The memory allocation per replica in GB. Must be between 0.5 and 8.0.
- `replicas` (Number) The number of desired replicas for the ClickPipe. Default is 1. The maximum value is 10.

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
# ClickPipes can be imported by specifying both service ID and clickpipe ID.
terraform import clickhouse_clickpipe.example xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```
